{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7064c92-b1ab-4486-9b17-28c303516059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f8ba01-4cba-4f79-af86-93138e8316b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in e:\\jupyter_notbook\\lib\\site-packages (0.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd5a9087-5aa3-4c27-b001-87d3224d3618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in e:\\jupyter_notbook\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in e:\\jupyter_notbook\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79d8ac58-10d0-4cf2-84a7-0e25c58f75e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import face_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cfb6d27-4422-4cb6-86c9-05f75bd2e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygame import mixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459ace8b-4e3d-4bf0-b13f-fe9f88bf4448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\heman\\majorproject\\dlib-19.24.99-cp312-cp312-win_amd64.whl\n",
      "dlib is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dlib-19.24.99-cp312-cp312-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "949dc0e5-c671-439d-868a-12a17d504a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4bae07a-f6e0-416e-8500-2405cb0770d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d95d8bb-a460-41e5-9d6d-847d286b73b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flag: 1, EAR: 0.24, MAR: 0.35\n",
      "Flag: 2, EAR: 0.24, MAR: 0.35\n",
      "Flag: 3, EAR: 0.22, MAR: 0.35\n",
      "Flag: 1, EAR: 0.23, MAR: 0.35\n",
      "Flag: 2, EAR: 0.23, MAR: 0.36\n",
      "Flag: 3, EAR: 0.23, MAR: 0.35\n",
      "Flag: 4, EAR: 0.23, MAR: 0.35\n",
      "Flag: 5, EAR: 0.24, MAR: 0.37\n",
      "Flag: 6, EAR: 0.20, MAR: 0.35\n",
      "Flag: 7, EAR: 0.20, MAR: 0.35\n",
      "Flag: 8, EAR: 0.22, MAR: 0.35\n",
      "Flag: 9, EAR: 0.22, MAR: 0.35\n",
      "Flag: 10, EAR: 0.25, MAR: 0.34\n",
      "Flag: 11, EAR: 0.22, MAR: 0.34\n",
      "Flag: 12, EAR: 0.22, MAR: 0.34\n",
      "Flag: 13, EAR: 0.21, MAR: 0.34\n",
      "Flag: 14, EAR: 0.23, MAR: 0.35\n",
      "Flag: 15, EAR: 0.22, MAR: 0.36\n",
      "Flag: 16, EAR: 0.22, MAR: 0.36\n",
      "Flag: 17, EAR: 0.21, MAR: 0.35\n",
      "Flag: 18, EAR: 0.21, MAR: 0.35\n",
      "Flag: 19, EAR: 0.24, MAR: 0.35\n",
      "Flag: 20, EAR: 0.24, MAR: 0.35\n",
      "Flag: 21, EAR: 0.24, MAR: 0.35\n",
      "Flag: 22, EAR: 0.18, MAR: 0.37\n",
      "Flag: 23, EAR: 0.18, MAR: 0.36\n",
      "Flag: 24, EAR: 0.18, MAR: 0.36\n",
      "Flag: 25, EAR: 0.20, MAR: 0.34\n",
      "Flag: 26, EAR: 0.21, MAR: 0.35\n",
      "Flag: 27, EAR: 0.20, MAR: 0.35\n",
      "Flag: 28, EAR: 0.20, MAR: 0.35\n",
      "Flag: 29, EAR: 0.21, MAR: 0.36\n",
      "Flag: 30, EAR: 0.21, MAR: 0.36\n",
      "Flag: 31, EAR: 0.19, MAR: 0.35\n",
      "Flag: 32, EAR: 0.19, MAR: 0.37\n",
      "Flag: 33, EAR: 0.19, MAR: 0.37\n",
      "Flag: 34, EAR: 0.19, MAR: 0.36\n",
      "Flag: 1, EAR: 0.25, MAR: 0.35\n",
      "Flag: 2, EAR: 0.20, MAR: 0.33\n",
      "Flag: 3, EAR: 0.20, MAR: 0.33\n",
      "Flag: 4, EAR: 0.19, MAR: 0.34\n",
      "Flag: 5, EAR: 0.23, MAR: 0.36\n",
      "Flag: 6, EAR: 0.23, MAR: 0.36\n",
      "Flag: 7, EAR: 0.21, MAR: 0.33\n",
      "Flag: 8, EAR: 0.21, MAR: 0.33\n",
      "Flag: 9, EAR: 0.24, MAR: 0.36\n",
      "Flag: 10, EAR: 0.20, MAR: 0.35\n",
      "Flag: 11, EAR: 0.22, MAR: 0.33\n",
      "Flag: 12, EAR: 0.22, MAR: 0.33\n",
      "Flag: 13, EAR: 0.24, MAR: 0.36\n",
      "Flag: 14, EAR: 0.18, MAR: 0.36\n",
      "Flag: 15, EAR: 0.18, MAR: 0.36\n",
      "Flag: 16, EAR: 0.21, MAR: 0.39\n",
      "Flag: 17, EAR: 0.19, MAR: 0.33\n",
      "Flag: 18, EAR: 0.19, MAR: 0.33\n",
      "Flag: 19, EAR: 0.22, MAR: 0.35\n",
      "Flag: 20, EAR: 0.22, MAR: 0.35\n",
      "Flag: 21, EAR: 0.21, MAR: 0.35\n",
      "Flag: 22, EAR: 0.18, MAR: 0.35\n",
      "Flag: 23, EAR: 0.18, MAR: 0.38\n",
      "Flag: 24, EAR: 0.18, MAR: 0.38\n",
      "Flag: 25, EAR: 0.20, MAR: 0.35\n",
      "Flag: 26, EAR: 0.19, MAR: 0.34\n",
      "Flag: 27, EAR: 0.19, MAR: 0.34\n",
      "Flag: 28, EAR: 0.21, MAR: 0.33\n",
      "Flag: 29, EAR: 0.21, MAR: 0.33\n",
      "Flag: 30, EAR: 0.18, MAR: 0.33\n",
      "Flag: 31, EAR: 0.18, MAR: 0.35\n",
      "Flag: 32, EAR: 0.18, MAR: 0.35\n",
      "Flag: 1, EAR: 0.34, MAR: 0.50\n",
      "Flag: 2, EAR: 0.34, MAR: 0.50\n",
      "Flag: 1, EAR: 0.23, MAR: 0.30\n",
      "Flag: 2, EAR: 0.23, MAR: 0.30\n",
      "Flag: 3, EAR: 0.23, MAR: 0.31\n",
      "Flag: 1, EAR: 0.25, MAR: 0.36\n",
      "Flag: 2, EAR: 0.21, MAR: 0.36\n",
      "Flag: 3, EAR: 0.21, MAR: 0.36\n",
      "Flag: 4, EAR: 0.21, MAR: 0.36\n",
      "Flag: 5, EAR: 0.21, MAR: 0.36\n",
      "Flag: 6, EAR: 0.21, MAR: 0.36\n",
      "Flag: 7, EAR: 0.20, MAR: 0.36\n",
      "Flag: 8, EAR: 0.20, MAR: 0.36\n",
      "Flag: 9, EAR: 0.18, MAR: 0.38\n",
      "Flag: 10, EAR: 0.21, MAR: 0.38\n",
      "Flag: 11, EAR: 0.20, MAR: 0.36\n",
      "Flag: 12, EAR: 0.20, MAR: 0.36\n",
      "Flag: 13, EAR: 0.18, MAR: 0.36\n",
      "Flag: 14, EAR: 0.18, MAR: 0.36\n",
      "Flag: 15, EAR: 0.18, MAR: 0.36\n",
      "Flag: 16, EAR: 0.22, MAR: 0.35\n",
      "Flag: 17, EAR: 0.22, MAR: 0.35\n",
      "Flag: 1, EAR: 0.24, MAR: 0.37\n",
      "Flag: 2, EAR: 0.18, MAR: 0.36\n",
      "Flag: 3, EAR: 0.18, MAR: 0.36\n",
      "Flag: 1, EAR: 0.20, MAR: 0.22\n",
      "Flag: 1, EAR: 0.23, MAR: 0.40\n",
      "Flag: 2, EAR: 0.23, MAR: 0.40\n",
      "Flag: 1, EAR: 0.32, MAR: 0.62\n",
      "Flag: 2, EAR: 0.32, MAR: 0.62\n",
      "Flag: 1, EAR: 0.21, MAR: 0.31\n",
      "Flag: 2, EAR: 0.17, MAR: 0.29\n",
      "Flag: 3, EAR: 0.17, MAR: 0.29\n",
      "Flag: 4, EAR: 0.19, MAR: 0.34\n",
      "Flag: 5, EAR: 0.19, MAR: 0.34\n",
      "Flag: 6, EAR: 0.18, MAR: 0.28\n",
      "\n",
      "--- Performance Metrics ---\n",
      "Total Frames with Face: 399\n",
      "Drowsy Frames Detected: 105\n",
      "Alerts Triggered: 28\n",
      "Detection Rate: 26.32%\n",
      "Alert Trigger Rate: 7.02%\n",
      "Approximate Accuracy: 80.70%\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from scipy.spatial import distance  # Used for calculating distances between points\n",
    "from imutils import face_utils  # Helps handle facial landmarks and shapes easily.\n",
    "from pygame import mixer  # For playing alert sound\n",
    "import imutils  # Utility functions for resizing images\n",
    "import dlib  # For face detection and landmarks\n",
    "import cv2  # OpenCV for image/video processing\n",
    "\n",
    "# Initialize audio mixer and load the alert sound\n",
    "mixer.init()  # Initialize the mixer (for playing sound).\n",
    "mixer.music.load(\"music.mp3\")  # Load the music file (the alert sound).\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])  # Vertical distance 1.  Calculates distance between two points on the eye.\n",
    "    B = distance.euclidean(eye[2], eye[4])  # Vertical distance 2.  Calculates another vertical distance.\n",
    "    C = distance.euclidean(eye[0], eye[3])  # Horizontal distance. Calculates the width of the eye.\n",
    "    ear = (A + B) / (2.0 * C)  # EAR formula.  Calculates the Eye Aspect Ratio using the distances.\n",
    "    return ear  # Returns the calculated EAR value.\n",
    "\n",
    "# Function to calculate Mouth Aspect Ratio (MAR)\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    A = distance.euclidean(mouth[2], mouth[10])  # Vertical distance 1. Distance between mouth points.\n",
    "    B = distance.euclidean(mouth[4], mouth[8])  # Vertical distance 2. Another distance.\n",
    "    C = distance.euclidean(mouth[0], mouth[6])  # Horizontal distance.  Width of the mouth.\n",
    "    mar = (A + B) / (2.0 * C)  # MAR formula.  Calculates Mouth Aspect Ratio.\n",
    "    return mar  # Returns the MAR.\n",
    "\n",
    "# Threshold values and check settings\n",
    "EYE_THRESH = 0.25  # EAR below this means eyes are likely closed.  A threshold for how much the eye is closed.\n",
    "MOUTH_THRESH = 0.5  # MAR above this means yawning. Threshold for how wide the mouth is.\n",
    "FRAME_CHECK = 20  # Number of continuous frames to confirm drowsiness.  How many frames the eyes/mouth must be in the drowsy state.\n",
    "\n",
    "# Load the face detector and facial landmarks model\n",
    "detect = dlib.get_frontal_face_detector()  #detects faces facing the camera.  Loads a pre-trained model to find faces.\n",
    "predict = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # detects 68 facial keypoints. Loads a model to find specific points on the face (eyes, mouth, etc.).\n",
    "\n",
    "# Get the indexes for eyes and mouth from the 68 landmarks\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"left_eye\"]  #we want to extract only Points around the eyes to calculate EAR.  Gets the numbers that correspond to the left eye points.\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"right_eye\"]\n",
    "(mStart, mEnd) = face_utils.FACIAL_LANDMARKS_68_IDXS[\"mouth\"]  #we want to extract only Points around the mouth to calculate MAR\n",
    "\n",
    "# Start capturing video from webcam\n",
    "cap = cv2.VideoCapture(0)  # Starts capturing video from the default webcam (0).\n",
    "flag = 0  # Counts how many continuous frames person seems drowsy.  A counter for how long the person has been drowsy.\n",
    "\n",
    "# Counters for performance evaluation\n",
    "total_frames = 0\n",
    "drowsy_frames = 0\n",
    "alert_triggered_frames = 0\n",
    "\n",
    "# Start reading video frames\n",
    "while True:  # Loop that continues until the program is stopped.\n",
    "    ret, frame = cap.read()  # Read a frame from webcam, ret is a boolean.  Reads one frame of video from the webcam.  'ret' indicates if the frame was read successfully.\n",
    "    frame = imutils.resize(frame, width=450)  # Resize frame for better performance.  Makes the frame smaller to speed up processing.\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert to grayscale.  Converts the color frame to grayscale (black and white), which is easier for face detection.\n",
    "    subjects = detect(gray, 0)  # Detect faces in the grayscale frame.  Uses the face detector to find faces in the image.\n",
    "\n",
    "    for subject in subjects:  # Loop that goes through each face found.\n",
    "        shape = predict(gray, subject)  # Predict landmarks on the face.  Finds the 68 facial landmark points on the face.\n",
    "        shape = face_utils.shape_to_np(shape)  # Convert to numpy array.  Converts the landmark points to a format that's easier to work with.\n",
    "        total_frames += 1  # Count total processed frames\n",
    "\n",
    "        # Get eye and mouth landmarks\n",
    "        leftEye = shape[lStart:lEnd]  # Extracts the landmark points for the left eye.\n",
    "        rightEye = shape[rStart:rEnd]  # Extracts points for the right eye.\n",
    "        mouth = shape[mStart:mEnd]  # Extracts points for the mouth.\n",
    "\n",
    "        # Calculate EAR and MAR\n",
    "        leftEAR = eye_aspect_ratio(leftEye)  # Calculates EAR for the left eye.\n",
    "        rightEAR = eye_aspect_ratio(rightEye)  # Calculates EAR for the right eye.\n",
    "        ear = (leftEAR + rightEAR) / 2.0  # Average EAR.  Averages the EAR from both eyes.\n",
    "        mar = mouth_aspect_ratio(mouth)  # MAR\n",
    "\n",
    "        # Draw contours around eyes and mouth\n",
    "        leftEyeHull = cv2.convexHull(leftEye)  # Finds the outline of the left eye points.\n",
    "        rightEyeHull = cv2.convexHull(rightEye)  # Outline of right eye.\n",
    "        mouthHull = cv2.convexHull(mouth)  # Outline of mouth.\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)  # Draws the outline on the image.\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [mouthHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "        # Drowsiness condition check\n",
    "        if ear < EYE_THRESH or mar > MOUTH_THRESH:  # If EAR is low or MAR is high (eyes closed or mouth open).\n",
    "            drowsy_frames += 1  # Person looks drowsy\n",
    "            flag += 1  # Increase continuous drowsy flag.  Increments the counter.\n",
    "            print(f\"Flag: {flag}, EAR: {ear:.2f}, MAR: {mar:.2f}\")\n",
    "\n",
    "            # If drowsy for FRAME_CHECK frames continuously, trigger alert\n",
    "            if flag >= FRAME_CHECK:  # If the person has been drowsy for long enough.\n",
    "                alert_triggered_frames += 1\n",
    "                cv2.putText(frame, \"*********************ALERT!*********************\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)  # Draws alert text on the image.\n",
    "                cv2.putText(frame, \"*********************ALERT!*********************\", (10, 325),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "                # Play alert sound only if it's not already playing\n",
    "                if not mixer.music.get_busy():\n",
    "                    mixer.music.play()  # Plays the alert sound.\n",
    "        else:\n",
    "            flag = 0  # Reset the flag if not drowsy.  Resets the counter if the person is not drowsy.\n",
    "\n",
    "            # Stop music if it's still playing\n",
    "            if mixer.music.get_busy():\n",
    "                mixer.music.stop()  # Stops the music if it was playing.\n",
    "\n",
    "    # Show the output frame with annotations\n",
    "    cv2.imshow(\"Frame\", frame)  # Displays the video frame with the eye and mouth outlines, and any alerts.\n",
    "\n",
    "    # Break the loop if user presses '1'\n",
    "    key = cv2.waitKey(1) & 0xFF  # Waits for a key press.\n",
    "    if key == ord(\"1\"):  # If the key '1' is pressed.\n",
    "        break  # Exit the loop.\n",
    "\n",
    "# Release camera and close all OpenCV windows\n",
    "cap.release()  # Releases the webcam.\n",
    "cv2.destroyAllWindows()  # Closes the display windows.\n",
    "\n",
    "# Print performance metrics\n",
    "if total_frames > 0:\n",
    "    print(f\"\\n--- Performance Metrics ---\")\n",
    "    print(f\"Total Frames with Face: {total_frames}\")\n",
    "    print(f\"Drowsy Frames Detected: {drowsy_frames}\")\n",
    "    print(f\"Alerts Triggered: {alert_triggered_frames}\")\n",
    "\n",
    "    detection_rate = (drowsy_frames / total_frames) * 100\n",
    "    alert_rate = (alert_triggered_frames / total_frames) * 100\n",
    "    non_drowsy_frames = total_frames - drowsy_frames\n",
    "    accuracy = ((non_drowsy_frames + alert_triggered_frames) / total_frames) * 100\n",
    "\n",
    "    print(f\"Detection Rate: {detection_rate:.2f}%\")\n",
    "    print(f\"Alert Trigger Rate: {alert_rate:.2f}%\")\n",
    "    print(f\"Approximate Accuracy: {accuracy:.2f}%\")\n",
    "else:\n",
    "    print(\"No frames with face detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c786c91b-19a1-4f68-9001-a39ef9c218da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
